version: "3.9"

services:

  autoscaler:
    image: sokrates1989/swarm-autoscaler:DEBUG-0.8.0
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${DATA_ROOT}/autoscaler_logs:/code/logs
    secrets:
      - "SWARM_MONITORING_AUTOSCALER_EMAIL_SENDER_PASSWORD"
      - "SWARM_MONITORING_AUTOSCALER_TELEGRAM_SENDER_BOT_TOKEN"
      - "SWARM_MONITORING_AUTOSCALER_STATECHECKER_SERVER_AUTH_TOKEN"
      - "SWARM_MONITORING_AUTOSCALER_STATECHECKER_TOOL_TOKEN"
      - "SWARM_MONITORING_AUTOSCALER_STATECHECKER_TELEGRAM_BOT_TOKEN"
    environment:
      # Logging.
      - LOG_LEVEL=${AUTOSCALER_GLOBAL_LOG_LEVEL}
      - LOG_STYLE=${AUTOSCALER_LOG_STYLE}
      - TIMEZONE=${AUTOSCALER_TIMEZONE}
      # Email status messages.
      - EMAIL_ENABLED=${AUTOSCALER_EMAIL_ENABLED}
      - EMAIL_SENDER_USER=${AUTOSCALER_EMAIL_SENDER_USER}
      - EMAIL_SENDER_PASSWORD_FILE=/run/secrets/SWARM_MONITORING_AUTOSCALER_EMAIL_SENDER_PASSWORD
      - EMAIL_SENDER_PASSWORD=${AUTOSCALER_EMAIL_SENDER_PASSWORD}
      - EMAIL_SENDER_HOST=${AUTOSCALER_EMAIL_SENDER_HOST}
      - EMAIL_SENDER_PORT=${AUTOSCALER_EMAIL_SENDER_PORT}
      - EMAIL_RECIPIENTS_IMPORTANT=${AUTOSCALER_EMAIL_RECIPIENTS_IMPORTANT}
      - EMAIL_RECIPIENTS_INFORMATION=${AUTOSCALER_EMAIL_RECIPIENTS_INFORMATION}
      - EMAIL_RECIPIENTS_VERBOSE=${AUTOSCALER_EMAIL_RECIPIENTS_VERBOSE}
      # Telegram status messages.
      - TELEGRAM_ENABLED=${AUTOSCALER_TELEGRAM_ENABLED}
      - TELEGRAM_SENDER_BOT_TOKEN_FILE=/run/secrets/SWARM_MONITORING_AUTOSCALER_TELEGRAM_SENDER_BOT_TOKEN
      - TELEGRAM_SENDER_BOT_TOKEN=${AUTOSCALER_TELEGRAM_SENDER_BOT_TOKEN}
      - TELEGRAM_RECIPIENTS_IMPORTANT=${AUTOSCALER_TELEGRAM_RECIPIENTS_IMPORTANT}
      - TELEGRAM_RECIPIENTS_INFORMATION=${AUTOSCALER_TELEGRAM_RECIPIENTS_INFORMATION}
      - TELEGRAM_RECIPIENTS_VERBOSE=${AUTOSCALER_TELEGRAM_RECIPIENTS_VERBOSE}
      # StateChecker.
      - STATECHECKER_ENABLED=${AUTOSCALER_STATECHECKER_ENABLED}
      - STATECHECKER_SERVER_STATE_CHECK_URL=${AUTOSCALER_STATECHECKER_SERVER_STATE_CHECK_URL}
      - STATECHECKER_SERVER_AUTHENTICATION_TOKEN=${AUTOSCALER_STATECHECKER_SERVER_AUTHENTICATION_TOKEN}
      - STATECHECKER_SERVER_AUTHENTICATION_TOKEN_FILE=/run/secrets/SWARM_MONITORING_AUTOSCALER_STATECHECKER_SERVER_AUTH_TOKEN
      - STATECHECKER_IS_BACKUP_FILE_CHECK=False
      - STATECHECKER_TOOL_NAME=${AUTOSCALER_STATECHECKER_TOOL_NAME}
      - STATECHECKER_TOOL_DESCRIPTION=${AUTOSCALER_STATECHECKER_TOOL_DESCRIPTION}
      - STATECHECKER_TOOL_TOKEN=${AUTOSCALER_STATECHECKER_TOOL_TOKEN}
      - STATECHECKER_TOOL_TOKEN_FILE=/run/secrets/SWARM_MONITORING_AUTOSCALER_STATECHECKER_TOOL_TOKEN
      - STATECHECKER_TOOL_FREQUENCY_IN_MINUTES=1
      # Statechecker Telegram status messages.
      - STATECHECKER_TELEGRAM_ENABLED=${AUTOSCALER_STATECHECKER_TELEGRAM_ENABLED}
      - STATECHECKER_TELEGRAM_BOT_TOKEN_File=/run/secrets/SWARM_MONITORING_AUTOSCALER_STATECHECKER_TELEGRAM_BOT_TOKEN
      - STATECHECKER_TELEGRAM_BOT_TOKEN=${AUTOSCALER_STATECHECKER_TELEGRAM_BOT_TOKEN}
      - STATECHECKER_TELEGRAM_ERROR_CHAT_ID=${AUTOSCALER_STATECHECKER_TELEGRAM_ERROR_CHAT_ID}
      - STATECHECKER_TELEGRAM_INFO_CHAT_ID=${AUTOSCALER_STATECHECKER_TELEGRAM_INFO_CHAT_ID}
    networks:
      - monitoring
    deploy:
      labels:
        # Cronjob to run this service every 15 seconds.
        # Only works in combination with https://github.com/crazy-max/swarm-cronjob or https://github.com/Sokrates1989/swarm-cronjob.git.
        - "swarm.cronjob.enable=true"
        - "swarm.cronjob.schedule=${AUTOSCALER_SCHEDULE}"
        - "swarm.cronjob.skip-running=false"
      replicas: 0
      restart_policy: 
        condition: none
      placement:
        constraints:
          - node.role == manager
          # If you cannot or do not want to use glusterfs or ceph: 
          # Set a label named "monitoring" with the value "true" on the node that you want to have autoscaler, grafana and prometheus running on.
          # REMEMBER, that if that node fails also autoscaler, grafana and prometheus will fail.
          # View README.md for more info about creating that label.
          # - node.labels.monitoring == true


  grafana:
    image: grafana/grafana:10.4.2
    ports:
      - target: 3000
        published: 3000
        protocol: tcp
        mode: ingress
    secrets:
      - "SWARM_MONITORING_GRAFANA_PASSWORD"
      - "SWARM_MONITORING_GRAFANA_SMTP_PASSWORD"
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
          # If you cannot or do not want to use glusterfs or ceph: 
          # Set a label named "monitoring" with the value "true" on the node that you want to have grafana running on.
          # REMEMBER, that if that node fails also grafana will fail.
          # View README.md for more info about creating that label.
          # - node.labels.monitoring == true
    volumes:
      - type: bind
        source: ${DATA_ROOT}/grafana_data
        target: /var/lib/grafana
      - type: bind
        source: ${DATA_ROOT}/grafana_config/dashboards.yml
        target: /etc/grafana/provisioning/dashboards/provisioning-dashboards.yaml
        read_only: true
      - type: bind
        source: ${DATA_ROOT}/grafana_config/datasources.yml
        target: /etc/grafana/provisioning/datasources/provisioning-datasources.yaml
        read_only: true
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}
      - GF_SECURITY_ADMIN_PASSWORD__FILE=/run/secrets/SWARM_MONITORING_GRAFANA_PASSWORD
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SMTP_ENABLED=${GRAFANA_SMTP_ENABLED}
      - GF_SMTP_HOST=${GRAFANA_SMTP_HOST}:${GRAFANA_SMTP_PORT}
      - GF_SMTP_USER=${GRAFANA_SMTP_USER}
      - GF_SMTP_FROM_ADDRESS=${GRAFANA_SMTP_FROM_ADDRESS}
      - GF_SMTP_PASSWORD__FILE=/run/secrets/SWARM_MONITORING_GRAFANA_SMTP_PASSWORD
    networks:
      - monitoring    


  prometheus:
    image: prom/prometheus:v2.51.2
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--log.level=error'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
          # If you cannot or do not want to use glusterfs or ceph: 
          # Set a label named "monitoring" with the value "true" on the node that you want to have grafana and prometheus running on.
          # REMEMBER, that if that node fails also grafana and prometheus will fail.
          # View README.md for more info about creating that label.
          # - node.labels.monitoring == true
    volumes:
      - type: bind
        source: ${DATA_ROOT}/prometheus_data
        target: /prometheus
      - type: bind
        source: ${DATA_ROOT}/prometheus.yml
        target: /etc/prometheus/prometheus.yml
    networks:
      - monitoring


  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    command: -logtostderr -docker_only
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    volumes:
      - type: bind
        source: /
        target: /rootfs
        read_only: true
      - type: bind
        source: /var/run
        target: /var/run
        read_only: true
      - type: bind
        source: /sys
        target: /sys
        read_only: true
      - type: bind
        source: /var/lib/docker
        target: /var/lib/docker
        read_only: true
      - type: bind
        source: /dev/disk
        target: /dev/disk
        read_only: true                        
    networks:
      - monitoring


  node-exporter:
    image: prom/node-exporter:v1.7.0
    command:
      - '--path.sysfs=/host/sys'
      - '--path.procfs=/host/proc'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      - '--no-collector.ipvs'
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    volumes:
      - type: bind
        source: /
        target: /rootfs
        read_only: true
      - type: bind
        source: /proc
        target: /host/proc
        read_only: true
      - type: bind
        source: /sys
        target: /host/sys
        read_only: true
    networks:
      - monitoring

    
networks:
  monitoring:
    driver: overlay

secrets:
  "SWARM_MONITORING_AUTOSCALER_EMAIL_SENDER_PASSWORD":
    external: true
  "SWARM_MONITORING_AUTOSCALER_TELEGRAM_SENDER_BOT_TOKEN":
    external: true
  "SWARM_MONITORING_AUTOSCALER_STATECHECKER_SERVER_AUTH_TOKEN":
    external: true
  "SWARM_MONITORING_AUTOSCALER_STATECHECKER_TOOL_TOKEN":
    external: true
  "SWARM_MONITORING_AUTOSCALER_STATECHECKER_TELEGRAM_BOT_TOKEN":
    external: true
  "SWARM_MONITORING_GRAFANA_PASSWORD":
    external: true
  "SWARM_MONITORING_GRAFANA_SMTP_PASSWORD":
    external: true
      
