version: "3.9"

services:

  autoscaler:
    image: sokrates1989/swarm-autoscaler:DEBUG-0.3.0
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${DATA_ROOT}/autoscaler_logs:/code/logs
    environment:
      - LOG_LEVEL=${AUTOSCALER_GLOBAL_LOG_LEVEL}
      - LOG_STYLE=${AUTOSCALER_LOG_STYLE}
      - TIMEZONE=${AUTOSCALER_TIMEZONE}
    networks:
      - monitoring
    deploy:
      labels:
        # Cronjob to run this service every 15 seconds.
        # Only works in combination with https://github.com/crazy-max/swarm-cronjob or https://github.com/Sokrates1989/swarm-cronjob.git.
        - "swarm.cronjob.enable=true"
        - "swarm.cronjob.schedule=*/15 * * * * *"
        - "swarm.cronjob.skip-running=false"
      replicas: 0
      restart_policy: 
        condition: none
      placement:
        constraints:
          - node.role == manager
          # If you cannot or do not want to use glusterfs or ceph: 
          # Set a label named "monitoring" with the value "true" on the node that you want to have autoscaler, grafana and prometheus running on.
          # REMEMBER, that if that node fails also autoscaler, grafana and prometheus will fail.
          # View README.md for more info about creating that label.
          # - node.labels.monitoring == true


  grafana:
    image: grafana/grafana:10.4.2
    secrets:
      - "SWARM_MONITORING_GRAFANA_PASSWORD"
      - "SWARM_MONITORING_GRAFANA_SMTP_PASSWORD"
    deploy:
      labels:
        - traefik.enable=true
        - traefik.constraint-label=traefik-public
        - traefik.http.services.swarm_monitoring_grafana.loadbalancer.server.port=3000
        - traefik.http.routers.swarm_monitoring_grafana.rule=Host(`${GRAFANA_URL}`)
        - traefik.http.routers.swarm_monitoring_grafana.entrypoints=https,http,web
        - traefik.http.routers.swarm_monitoring_grafana.tls=true
        - traefik.http.routers.swarm_monitoring_grafana.tls.certresolver=le
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
          # If you cannot or do not want to use glusterfs or ceph: 
          # Set a label named "monitoring" with the value "true" on the node that you want to have autoscaler, grafana and prometheus running on.
          # REMEMBER, that if that node fails also autoscaler, grafana and prometheus will fail.
          # View README.md for more info about creating that label.
          # - node.labels.monitoring == true
    volumes:
      - type: bind
        source: ${DATA_ROOT}/grafana_data
        target: /var/lib/grafana
      - type: bind
        source: ${DATA_ROOT}/grafana_config/dashboards.yml
        target: /etc/grafana/provisioning/dashboards/provisioning-dashboards.yaml
        read_only: true
      - type: bind
        source: ${DATA_ROOT}/grafana_config/datasources.yml
        target: /etc/grafana/provisioning/datasources/provisioning-datasources.yaml
        read_only: true
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}
      - GF_SECURITY_ADMIN_PASSWORD__FILE=/run/secrets/SWARM_MONITORING_GRAFANA_PASSWORD
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SMTP_ENABLED=${GRAFANA_SMTP_ENABLED}
      - GF_SMTP_HOST=${GRAFANA_SMTP_HOST}:${GRAFANA_SMTP_PORT}
      - GF_SMTP_USER=${GRAFANA_SMTP_USER}
      - GF_SMTP_FROM_ADDRESS=${GRAFANA_SMTP_FROM_ADDRESS}
      # - GF_SMTP_PASSWORD=${GRAFANA_SMTP_PASSWORD} # Only use this for debugging and testing
      - GF_SMTP_PASSWORD__FILE=/run/secrets/SWARM_MONITORING_GRAFANA_SMTP_PASSWORD
    networks:
      - monitoring 
      - traefik 


  prometheus:
    image: prom/prometheus:v2.51.2
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--log.level=error'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
          # If you cannot or do not want to use glusterfs or ceph: 
          # Set a label named "monitoring" with the value "true" on the node that you want to have grafana and prometheus running on.
          # REMEMBER, that if that node fails also grafana and prometheus will fail.
          # View README.md for more info about creating that label.
          # - node.labels.monitoring == true
    volumes:
      - type: bind
        source: ${DATA_ROOT}/prometheus_data
        target: /prometheus
      - type: bind
        source: ${DATA_ROOT}/prometheus.yml
        target: /etc/prometheus/prometheus.yml
    networks:
      - monitoring


  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    command: -logtostderr -docker_only
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    volumes:
      - type: bind
        source: /
        target: /rootfs
        read_only: true
      - type: bind
        source: /var/run
        target: /var/run
        read_only: true
      - type: bind
        source: /sys
        target: /sys
        read_only: true
      - type: bind
        source: /var/lib/docker
        target: /var/lib/docker
        read_only: true
      - type: bind
        source: /dev/disk
        target: /dev/disk
        read_only: true                        
    networks:
      - monitoring


  node-exporter:
    image: prom/node-exporter:v1.7.0
    command:
      - '--path.sysfs=/host/sys'
      - '--path.procfs=/host/proc'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      - '--no-collector.ipvs'
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    volumes:
      - type: bind
        source: /
        target: /rootfs
        read_only: true
      - type: bind
        source: /proc
        target: /host/proc
        read_only: true
      - type: bind
        source: /sys
        target: /host/sys
        read_only: true
    networks:
      - monitoring


networks:
  monitoring:
    driver: overlay
  traefik:
    external: true

secrets:
  "SWARM_MONITORING_GRAFANA_PASSWORD":
    external: true
  "SWARM_MONITORING_GRAFANA_SMTP_PASSWORD":
    external: true
